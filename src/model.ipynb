{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_scaled_error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvLSTMCell** is the basic building block of the ConvLSTM model. It defines a single ConvLSTM cell that takes as input a tensor and a tuple representing the current state of the cell, and outputs a tensor and a tuple representing the next state of the cell. The cell includes a convolutional layer and four gates (input, forget, output, and cell gates) that are used to update the cell state and hidden state. The output tensor is the hidden state of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvLSTM** is the main class that creates a ConvLSTM model with multiple layers. It takes as input a tensor and an optional tuple representing the initial hidden state of the model, and returns a tuple of two lists. The first list contains the hidden states of each layer at each timestep, and the second list contains the final hidden states and cell states of each layer. The model includes multiple ConvLSTMCell cells that are stacked on top of each other to create a deeper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    channels = x.shape[1]\n",
    "    width = x.shape[2]\n",
    "    height = x.shape[3]\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row,window_size, channels, width, height), strides=(x.strides[0],x.strides[0],x.strides[1],x.strides[2],x.strides[3]))\n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "def prepare_data(normalized_data_close_price):\n",
    "    data_x, _ = prepare_data_x(normalized_data_close_price, window_size=12)\n",
    "    data_y = prepare_data_y(normalized_data_close_price, window_size=12)\n",
    "\n",
    "    # split dataset\n",
    "\n",
    "    split_index = int(data_y.shape[0]*0.8)\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "\n",
    "    return split_index, data_x_train, data_y_train, data_x_val, data_y_val\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "    \n",
    "class Normalizer():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mu = np.mean(x, axis=(0), keepdims=True)\n",
    "        self.sd = np.std(x, axis=(0), keepdims=True)\n",
    "        normalized_x = (x - self.mu)/self.sd\n",
    "        return normalized_x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return (x*self.sd) + self.mu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m train_model(model, dataloader, criterion, optimizer, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m predicted_val \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m _, output \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m output_inner \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(seq_len):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     h, c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcell_list[layer_idx](input_tensor\u001b[39m=\u001b[39;49mcur_layer_input[:, t, :, :, :],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m                                      cur_state\u001b[39m=\u001b[39;49m[h, c])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     output_inner\u001b[39m.\u001b[39mappend(h)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m layer_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(output_inner, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m i \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(cc_i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(cc_f)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m o \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msigmoid(cc_o)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m g \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(cc_g)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m c_next \u001b[39m=\u001b[39m f \u001b[39m*\u001b[39m c_cur \u001b[39m+\u001b[39m i \u001b[39m*\u001b[39m g\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        # extract date string from row and parse it as a datetime objsaDect\n",
    "        date_str = row[1]\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "        features = row[3:].values.astype(np.float32)\n",
    "        label = row[0].astype(np.float32)\n",
    "        sample = {'date_begin': date, 'features': self.transform(features), 'label': label}\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Define the training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for idx, (x,y) in enumerate(dataloader,0):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, output = model(x)\n",
    "            loss = criterion(output[0][0], y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch %d, loss: %.3f' % (epoch + 1, running_loss / len(dataloader)))\n",
    "\n",
    "# Load the data\n",
    "dataset = MyDataset('./data/ocorrencias_final.csv')\n",
    "data_grid = np.load('data_grid.npy') \n",
    "data_grid = data_grid.reshape(965, 22, 16, 4)\n",
    "data_grid = np.swapaxes(data_grid, 3, 1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "split_index, data_x_train, data_y_train, data_x_val, data_y_val = prepare_data(data_grid)\n",
    "\n",
    "\n",
    "dataset_train = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "dataset_val = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "\n",
    "data_x_train = data_x_train.astype(np.float32)\n",
    "data_y_train = data_y_train.astype(np.float32)\n",
    "\n",
    "dataloader = DataLoader(dataset_train, batch_size=1, shuffle=False, num_workers=12)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=12)\n",
    "\n",
    "# Create the model, optimizer and loss function\n",
    "model = ConvLSTM(input_dim=4, hidden_dim=[64, 64, 4], kernel_size=(3, 3), num_layers=3, batch_first=True, bias=True, return_all_layers=False)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.004)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predicted_val = []\n",
    "\n",
    "for idx, (x, y) in enumerate(val_dataloader):\n",
    "    x = x.to(device)\n",
    "    _,out = model(x)\n",
    "    out = out[0][0].cpu()\n",
    "    predicted_val.append(out.detach().numpy()[0])\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model2.pth')\n",
    "predicted_val = np.array(predicted_val)\n",
    "print(predicted_val.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "data_y_val = data_y_val.reshape(191, 1408)\n",
    "predicted_val = predicted_val.reshape(191, 1408)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(data_y_val, predicted_val))\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 4, 16, 22)\n",
      "(191, 4, 16, 22)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m predicted_val \u001b[39m=\u001b[39m predicted_val\u001b[39m.\u001b[39mreshape(\u001b[39m191\u001b[39m, \u001b[39m1408\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data_y_train \u001b[39m=\u001b[39m data_y_train\u001b[39m.\u001b[39mreshape(\u001b[39m762\u001b[39m, \u001b[39m1408\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(mean_squared_error(data_y_val, predicted_val))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m mase \u001b[39m=\u001b[39m mean_absolute_scaled_error(y_true\u001b[39m=\u001b[39mdata_y_val, y_pred\u001b[39m=\u001b[39mpredicted_val, y_train\u001b[39m=\u001b[39mdata_y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(rmse)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/sklearn/metrics/_regression.py:442\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_squared_error\u001b[39m(\n\u001b[1;32m    383\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    384\u001b[0m ):\n\u001b[1;32m    385\u001b[0m     \u001b[39m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m    0.825...\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    443\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    446\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage((y_true \u001b[39m-\u001b[39m y_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weights\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/sklearn/metrics/_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    101\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m--> 102\u001b[0m y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    105\u001b[0m     y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[1;32m    900\u001b[0m             array,\n\u001b[1;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    904\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "predicted_val = np.array(predicted_val)\n",
    "print(predicted_val.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "data_y_val = data_y_val.reshape(191, 1408)\n",
    "predicted_val = predicted_val.reshape(191, 1408)\n",
    "data_y_train = data_y_train.reshape(762, 1408)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(data_y_val, predicted_val))\n",
    "mase = mean_absolute_scaled_error(y_true=data_y_val, y_pred=predicted_val, y_train=data_y_train)\n",
    "print(rmse)\n",
    "print(mase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.86767056e-07 -7.62678393e-11 -4.40937634e-11 -3.97206264e-07\n",
      "  -4.65746089e-05  5.68828778e-03  8.19952220e-06  1.05131476e-04\n",
      "   2.31512566e-03  1.43417667e-04 -5.05817006e-04 -2.32556502e-07\n",
      "  -5.79607518e-09 -2.14864867e-05 -6.24179595e-15 -3.38032532e-15\n",
      "  -1.22259516e-12 -2.05907819e-10 -7.92174590e-11 -5.88010334e-06\n",
      "   3.49845286e-09 -3.51644030e-06]\n",
      " [-8.76098216e-09 -2.78596283e-17 -2.13244060e-15 -6.99649297e-11\n",
      "  -8.56295228e-05 -2.29013473e-04  9.99999881e-01  1.00000000e+00\n",
      "   1.00000000e+00  9.99986410e-01  2.00018345e-04 -6.42973869e-11\n",
      "  -3.01189846e-13 -1.20678436e-14 -2.71970756e-19 -9.25635818e-17\n",
      "  -5.12792489e-17 -3.05491366e-17 -1.20262962e-17 -9.95089081e-12\n",
      "  -3.05241478e-16 -6.61366739e-10]\n",
      " [-8.90262483e-04 -9.76788650e-10 -1.44485259e-08 -3.61932575e-06\n",
      "  -9.05487454e-04  9.99766290e-01  1.00000000e+00  9.99999642e-01\n",
      "   9.99998331e-01  9.99999881e-01  9.99994159e-01  6.04523339e-05\n",
      "   4.84539283e-04  1.00503801e-08 -7.57900338e-08 -5.39941182e-11\n",
      "  -6.19148552e-11 -6.24472609e-12 -9.29492094e-10 -1.50815225e-07\n",
      "  -2.59656120e-12 -3.27865557e-09]\n",
      " [ 9.99339521e-01  7.86700298e-07  1.48989158e-02  9.99070108e-01\n",
      "   4.54806909e-03  3.80638964e-03  9.99999881e-01  9.99993563e-01\n",
      "   9.99999642e-01  9.99999642e-01  9.99998569e-01  1.00000000e+00\n",
      "   9.99993563e-01  1.34943332e-06  4.23805504e-05  5.75668491e-05\n",
      "   2.81466127e-05  2.53029102e-05  3.60121974e-03  7.57483482e-01\n",
      "   2.77589223e-08  4.57756614e-05]\n",
      " [ 1.00000000e+00  9.99984741e-01  9.99999642e-01  9.99989152e-01\n",
      "   9.99929428e-01  8.04545641e-01  9.99997020e-01  9.99977112e-01\n",
      "   9.99975562e-01  9.99999046e-01  9.99993563e-01  9.99998450e-01\n",
      "   9.99998331e-01  9.99999285e-01  9.99999404e-01  9.99995470e-01\n",
      "   1.00000000e+00  9.99999881e-01  9.99999881e-01  9.99999881e-01\n",
      "   9.99994397e-01  7.96794484e-05]\n",
      " [ 1.00000000e+00  9.99372423e-01  9.75023150e-01  9.47383702e-01\n",
      "   9.99806345e-01  9.99894857e-01  9.99986291e-01  9.99997616e-01\n",
      "   9.99994755e-01  9.99999523e-01  9.99999762e-01  9.99993682e-01\n",
      "   9.99998093e-01  9.99989510e-01  9.99970794e-01  9.99994397e-01\n",
      "   9.99997854e-01  9.99995708e-01  9.99999166e-01  9.99999881e-01\n",
      "   9.99996662e-01  4.42023775e-05]\n",
      " [ 1.00000000e+00  9.99053061e-01  9.99502063e-01  9.99837041e-01\n",
      "   9.99972701e-01  8.43518674e-01  9.99858201e-01  9.90723670e-01\n",
      "   9.99993563e-01  9.99997020e-01  9.99998927e-01  9.99986768e-01\n",
      "   9.99991894e-01  9.99989510e-01  9.99993443e-01  9.99999046e-01\n",
      "   9.99999881e-01  1.00000000e+00  9.99991655e-01  9.43940520e-01\n",
      "   9.91151392e-01 -5.92057477e-05]\n",
      " [ 9.99999642e-01  8.01078737e-01  9.99769092e-01  9.91553962e-01\n",
      "   9.99987245e-01  9.99599993e-01  9.99999523e-01  9.91804004e-01\n",
      "   9.99798119e-01  9.99982476e-01  9.99999166e-01  9.99995470e-01\n",
      "   9.99999881e-01  9.99995351e-01  9.99999762e-01  9.99996781e-01\n",
      "   1.00000000e+00  9.99998569e-01  9.99995112e-01  3.07613701e-01\n",
      "   4.55443524e-06 -2.52607435e-07]\n",
      " [ 9.99998689e-01  3.64089727e-01  7.67333329e-01  7.44576693e-01\n",
      "   9.99737084e-01  9.95516360e-01  9.98478115e-01  9.99830365e-01\n",
      "   9.99964237e-01  9.99979496e-01  9.99881387e-01  9.99969482e-01\n",
      "   9.99614954e-01  9.99996543e-01  9.99976635e-01  9.99996901e-01\n",
      "   1.00000000e+00  9.99999642e-01  9.99776423e-01  9.99673605e-01\n",
      "   4.63519463e-08 -7.75753847e-07]\n",
      " [ 9.99278963e-01  7.58878589e-01  6.04686439e-01  8.18145335e-01\n",
      "   9.99989629e-01  9.99983668e-01  9.99847889e-01  9.99904513e-01\n",
      "   9.99729931e-01  9.99999762e-01  1.77334145e-01  9.99999166e-01\n",
      "   9.99676824e-01  9.99999762e-01  9.99926209e-01  9.99414563e-01\n",
      "   9.99998927e-01  9.99991059e-01  9.99975502e-01  9.79732037e-01\n",
      "   7.96360200e-10 -2.60720991e-08]\n",
      " [ 1.02414288e-05  3.92155198e-05  4.40225705e-08  1.12062797e-03\n",
      "   7.10790396e-01  9.85836744e-01  6.19930029e-01  9.99999762e-01\n",
      "   9.97612715e-01  6.63692713e-01  3.63748497e-03  7.78760135e-01\n",
      "   9.97967899e-01  9.99997854e-01  9.99817193e-01  9.99857664e-01\n",
      "   9.99989390e-01  9.99996066e-01  9.99984860e-01  4.32877034e-01\n",
      "   4.35954576e-07 -1.12865841e-07]\n",
      " [-3.78705609e-11 -1.96112463e-17 -1.73400747e-06 -6.98062808e-09\n",
      "   3.90324881e-03  3.70376743e-03  2.90492433e-04  5.60863256e-01\n",
      "   9.99115884e-01  1.15533755e-03  2.26953276e-07  3.50490154e-04\n",
      "   2.43574567e-02  9.63328302e-01  9.98323083e-01  9.93960679e-01\n",
      "   9.99079704e-01  9.99996781e-01  9.99999762e-01  7.58527935e-01\n",
      "   5.13812566e-08 -5.35065169e-07]\n",
      " [-9.12495161e-17 -2.62039272e-22 -8.94486314e-17 -1.83970305e-15\n",
      "  -6.11333206e-12 -6.54026380e-08 -8.14759016e-09  6.33724403e-06\n",
      "   8.52699450e-05 -5.52026904e-08 -9.93117602e-11 -3.60661667e-10\n",
      "  -1.47489201e-10  3.75698204e-04  4.44041789e-01  4.05934639e-03\n",
      "   3.89465898e-01  9.99999166e-01  9.99838352e-01  1.00000000e+00\n",
      "   1.12565471e-10 -2.19404299e-08]\n",
      " [-2.51012115e-15 -8.51721242e-18 -2.53044978e-16 -2.98547603e-19\n",
      "  -7.42747496e-19 -2.14998138e-17 -1.51316904e-12 -1.50427054e-14\n",
      "  -3.30720524e-14 -2.34471513e-13 -1.89729024e-13 -5.50767080e-16\n",
      "  -2.07387146e-11 -7.46767492e-12  8.62266404e-07 -9.14966503e-09\n",
      "   1.19854864e-11  7.38639951e-01  7.62761056e-01  1.64411291e-02\n",
      "   5.98762576e-14 -1.30502167e-06]\n",
      " [-5.53526396e-16 -3.86101569e-19 -2.58366479e-14 -1.72431175e-17\n",
      "  -5.64487117e-18 -4.04572880e-17 -1.94980039e-14 -7.98763517e-17\n",
      "  -6.19773346e-17 -1.11874163e-14 -3.28393932e-15 -2.24469314e-18\n",
      "  -1.30049691e-15 -6.13707597e-17 -7.55373056e-15 -1.50809448e-10\n",
      "  -1.68093664e-10 -2.52263314e-11  5.20678699e-01 -6.13597351e-09\n",
      "  -1.49418835e-16 -6.35057967e-03]\n",
      " [-1.46673642e-07 -1.46497977e-18 -3.25966251e-15 -1.23871874e-13\n",
      "  -8.83146875e-15 -9.38681771e-12 -6.02958217e-09 -4.38427947e-14\n",
      "  -6.03457909e-11 -2.54643702e-12 -4.14350069e-07 -3.79773817e-11\n",
      "  -2.38582274e-08 -3.32581212e-13 -4.13320392e-13 -2.32997122e-10\n",
      "  -5.46704956e-12 -9.25797741e-13 -1.74338853e-12 -1.47639842e-14\n",
      "  -3.96847421e-19 -6.31160635e-09]]\n",
      "(191, 4, 16, 22)\n",
      "(191, 1408)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read and build the neural network\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import rotate\n",
    "import imageio\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "\n",
    "model = ConvLSTM(input_dim=4, hidden_dim=[64, 64, 4], kernel_size=(3, 3), num_layers=3, batch_first=True, bias=True, return_all_layers=False)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "predicted_val = []\n",
    "\n",
    "for idx, (x, y) in enumerate(val_dataloader):\n",
    "    x = x.to(device)\n",
    "    _,out = model(x)\n",
    "    out = out[0][0].cpu()\n",
    "    predicted_val.append(out.detach().numpy()[0])\n",
    "\n",
    "# print heatmap each predicted_val\n",
    "predicted_val = np.array(predicted_val)\n",
    "print(predicted_val[0][0])\n",
    "#invert the 2d\n",
    "\n",
    "x = predicted_val[0][0]\n",
    "x = rotate(x, angle=90)\n",
    "\n",
    "plt.imshow(x, cmap='hot', interpolation='nearest')\n",
    "\n",
    "#increase the resolution of the heatmap\n",
    "new_size = (x.shape[0]*2, x.shape[1]*2)\n",
    "new_img = ndimage.zoom(x, zoom=(2, 2), order=1)\n",
    "\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(new_img, cmap='hot', interpolation='nearest')\n",
    "plt.savefig('test.png', transparent=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(predicted_val.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78620/2684157384.py:21: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread('heatmap.png'))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = []\n",
    "\n",
    "feature_num = 1\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 191):\n",
    "    x = predicted_val[i][feature_num]\n",
    "    x = rotate(x, angle=90)\n",
    "    new_size = (x.shape[0]*2, x.shape[1]*2)\n",
    "    new_img = ndimage.zoom(x, zoom=(2, 2), order=1) \n",
    "    plt.imshow(new_img, cmap='hot_r', interpolation='nearest')\n",
    "    #add a colorbar\n",
    "    #add a progress bar to the image of the frames\n",
    "    #plt.text(0.5, 0.5, str(i), fontsize=18, ha='center')\n",
    "    plt.title('Heatmap of predicted values')\n",
    "    \n",
    "\n",
    "\n",
    "    plt.savefig('heatmap.png')\n",
    "    images.append(imageio.imread('heatmap.png'))\n",
    "\n",
    "imageio.mimsave(f'../plots/heatmap_inv_feature{feature_num}.gif', images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
