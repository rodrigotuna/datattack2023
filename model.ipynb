{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_scaled_error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvLSTMCell** is the basic building block of the ConvLSTM model. It defines a single ConvLSTM cell that takes as input a tensor and a tuple representing the current state of the cell, and outputs a tensor and a tuple representing the next state of the cell. The cell includes a convolutional layer and four gates (input, forget, output, and cell gates) that are used to update the cell state and hidden state. The output tensor is the hidden state of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvLSTM** is the main class that creates a ConvLSTM model with multiple layers. It takes as input a tensor and an optional tuple representing the initial hidden state of the model, and returns a tuple of two lists. The first list contains the hidden states of each layer at each timestep, and the second list contains the final hidden states and cell states of each layer. The model includes multiple ConvLSTMCell cells that are stacked on top of each other to create a deeper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    channels = x.shape[1]\n",
    "    width = x.shape[2]\n",
    "    height = x.shape[3]\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row,window_size, channels, width, height), strides=(x.strides[0],x.strides[0],x.strides[1],x.strides[2],x.strides[3]))\n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "def prepare_data(normalized_data_close_price):\n",
    "    data_x, _ = prepare_data_x(normalized_data_close_price, window_size=12)\n",
    "    data_y = prepare_data_y(normalized_data_close_price, window_size=12)\n",
    "\n",
    "    # split dataset\n",
    "\n",
    "    split_index = int(data_y.shape[0]*0.8)\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "\n",
    "    return split_index, data_x_train, data_y_train, data_x_val, data_y_val\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "    \n",
    "class Normalizer():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mu = np.mean(x, axis=(0), keepdims=True)\n",
    "        self.sd = np.std(x, axis=(0), keepdims=True)\n",
    "        normalized_x = (x - self.mu)/self.sd\n",
    "        return normalized_x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return (x*self.sd) + self.mu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m train_model(model, dataloader, criterion, optimizer, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m predicted_val \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m _, output \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m output_inner \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(seq_len):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     h, c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcell_list[layer_idx](input_tensor\u001b[39m=\u001b[39;49mcur_layer_input[:, t, :, :, :],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m                                      cur_state\u001b[39m=\u001b[39;49m[h, c])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     output_inner\u001b[39m.\u001b[39mappend(h)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m layer_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(output_inner, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m i \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(cc_i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(cc_f)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m o \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msigmoid(cc_o)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m g \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(cc_g)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m c_next \u001b[39m=\u001b[39m f \u001b[39m*\u001b[39m c_cur \u001b[39m+\u001b[39m i \u001b[39m*\u001b[39m g\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        # extract date string from row and parse it as a datetime objsaDect\n",
    "        date_str = row[1]\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "        features = row[3:].values.astype(np.float32)\n",
    "        label = row[0].astype(np.float32)\n",
    "        sample = {'date_begin': date, 'features': self.transform(features), 'label': label}\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Define the training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for idx, (x,y) in enumerate(dataloader,0):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, output = model(x)\n",
    "            loss = criterion(output[0][0], y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch %d, loss: %.3f' % (epoch + 1, running_loss / len(dataloader)))\n",
    "\n",
    "# Load the data\n",
    "dataset = MyDataset('./data/ocorrencias_final.csv')\n",
    "data_grid = np.load('data_grid.npy') \n",
    "data_grid = data_grid.reshape(965, 22, 16, 4)\n",
    "data_grid = np.swapaxes(data_grid, 3, 1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "split_index, data_x_train, data_y_train, data_x_val, data_y_val = prepare_data(data_grid)\n",
    "\n",
    "\n",
    "dataset_train = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "dataset_val = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "\n",
    "data_x_train = data_x_train.astype(np.float32)\n",
    "data_y_train = data_y_train.astype(np.float32)\n",
    "\n",
    "dataloader = DataLoader(dataset_train, batch_size=1, shuffle=False, num_workers=12)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=12)\n",
    "\n",
    "# Create the model, optimizer and loss function\n",
    "model = ConvLSTM(input_dim=4, hidden_dim=[64, 64, 4], kernel_size=(3, 3), num_layers=3, batch_first=True, bias=True, return_all_layers=False)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.004)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predicted_val = []\n",
    "\n",
    "for idx, (x, y) in enumerate(val_dataloader):\n",
    "    x = x.to(device)\n",
    "    _,out = model(x)\n",
    "    out = out[0][0].cpu()\n",
    "    predicted_val.append(out.detach().numpy()[0])\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model2.pth')\n",
    "predicted_val = np.array(predicted_val)\n",
    "print(predicted_val.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "data_y_val = data_y_val.reshape(191, 1408)\n",
    "predicted_val = predicted_val.reshape(191, 1408)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(data_y_val, predicted_val))\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 4, 16, 22)\n",
      "(191, 4, 16, 22)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m predicted_val \u001b[39m=\u001b[39m predicted_val\u001b[39m.\u001b[39mreshape(\u001b[39m191\u001b[39m, \u001b[39m1408\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data_y_train \u001b[39m=\u001b[39m data_y_train\u001b[39m.\u001b[39mreshape(\u001b[39m762\u001b[39m, \u001b[39m1408\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(mean_squared_error(data_y_val, predicted_val))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m mase \u001b[39m=\u001b[39m mean_absolute_scaled_error(y_true\u001b[39m=\u001b[39mdata_y_val, y_pred\u001b[39m=\u001b[39mpredicted_val, y_train\u001b[39m=\u001b[39mdata_y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(rmse)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/sklearn/metrics/_regression.py:442\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_squared_error\u001b[39m(\n\u001b[1;32m    383\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    384\u001b[0m ):\n\u001b[1;32m    385\u001b[0m     \u001b[39m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m    0.825...\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    443\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    446\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage((y_true \u001b[39m-\u001b[39m y_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weights\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/sklearn/metrics/_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    101\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m--> 102\u001b[0m y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    105\u001b[0m     y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[1;32m    900\u001b[0m             array,\n\u001b[1;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    904\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "predicted_val = np.array(predicted_val)\n",
    "print(predicted_val.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "data_y_val = data_y_val.reshape(191, 1408)\n",
    "predicted_val = predicted_val.reshape(191, 1408)\n",
    "data_y_train = data_y_train.reshape(762, 1408)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(data_y_val, predicted_val))\n",
    "mase = mean_absolute_scaled_error(y_true=data_y_val, y_pred=predicted_val, y_train=data_y_train)\n",
    "print(rmse)\n",
    "print(mase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.86767056e-07 -7.62678393e-11 -4.40937634e-11 -3.97206264e-07\n",
      "  -4.65746089e-05  5.68828778e-03  8.19952220e-06  1.05131476e-04\n",
      "   2.31512566e-03  1.43417667e-04 -5.05817006e-04 -2.32556502e-07\n",
      "  -5.79607518e-09 -2.14864867e-05 -6.24179595e-15 -3.38032532e-15\n",
      "  -1.22259516e-12 -2.05907819e-10 -7.92174590e-11 -5.88010334e-06\n",
      "   3.49845286e-09 -3.51644030e-06]\n",
      " [-8.76098216e-09 -2.78596283e-17 -2.13244060e-15 -6.99649297e-11\n",
      "  -8.56295228e-05 -2.29013473e-04  9.99999881e-01  1.00000000e+00\n",
      "   1.00000000e+00  9.99986410e-01  2.00018345e-04 -6.42973869e-11\n",
      "  -3.01189846e-13 -1.20678436e-14 -2.71970756e-19 -9.25635818e-17\n",
      "  -5.12792489e-17 -3.05491366e-17 -1.20262962e-17 -9.95089081e-12\n",
      "  -3.05241478e-16 -6.61366739e-10]\n",
      " [-8.90262483e-04 -9.76788650e-10 -1.44485259e-08 -3.61932575e-06\n",
      "  -9.05487454e-04  9.99766290e-01  1.00000000e+00  9.99999642e-01\n",
      "   9.99998331e-01  9.99999881e-01  9.99994159e-01  6.04523339e-05\n",
      "   4.84539283e-04  1.00503801e-08 -7.57900338e-08 -5.39941182e-11\n",
      "  -6.19148552e-11 -6.24472609e-12 -9.29492094e-10 -1.50815225e-07\n",
      "  -2.59656120e-12 -3.27865557e-09]\n",
      " [ 9.99339521e-01  7.86700298e-07  1.48989158e-02  9.99070108e-01\n",
      "   4.54806909e-03  3.80638964e-03  9.99999881e-01  9.99993563e-01\n",
      "   9.99999642e-01  9.99999642e-01  9.99998569e-01  1.00000000e+00\n",
      "   9.99993563e-01  1.34943332e-06  4.23805504e-05  5.75668491e-05\n",
      "   2.81466127e-05  2.53029102e-05  3.60121974e-03  7.57483482e-01\n",
      "   2.77589223e-08  4.57756614e-05]\n",
      " [ 1.00000000e+00  9.99984741e-01  9.99999642e-01  9.99989152e-01\n",
      "   9.99929428e-01  8.04545641e-01  9.99997020e-01  9.99977112e-01\n",
      "   9.99975562e-01  9.99999046e-01  9.99993563e-01  9.99998450e-01\n",
      "   9.99998331e-01  9.99999285e-01  9.99999404e-01  9.99995470e-01\n",
      "   1.00000000e+00  9.99999881e-01  9.99999881e-01  9.99999881e-01\n",
      "   9.99994397e-01  7.96794484e-05]\n",
      " [ 1.00000000e+00  9.99372423e-01  9.75023150e-01  9.47383702e-01\n",
      "   9.99806345e-01  9.99894857e-01  9.99986291e-01  9.99997616e-01\n",
      "   9.99994755e-01  9.99999523e-01  9.99999762e-01  9.99993682e-01\n",
      "   9.99998093e-01  9.99989510e-01  9.99970794e-01  9.99994397e-01\n",
      "   9.99997854e-01  9.99995708e-01  9.99999166e-01  9.99999881e-01\n",
      "   9.99996662e-01  4.42023775e-05]\n",
      " [ 1.00000000e+00  9.99053061e-01  9.99502063e-01  9.99837041e-01\n",
      "   9.99972701e-01  8.43518674e-01  9.99858201e-01  9.90723670e-01\n",
      "   9.99993563e-01  9.99997020e-01  9.99998927e-01  9.99986768e-01\n",
      "   9.99991894e-01  9.99989510e-01  9.99993443e-01  9.99999046e-01\n",
      "   9.99999881e-01  1.00000000e+00  9.99991655e-01  9.43940520e-01\n",
      "   9.91151392e-01 -5.92057477e-05]\n",
      " [ 9.99999642e-01  8.01078737e-01  9.99769092e-01  9.91553962e-01\n",
      "   9.99987245e-01  9.99599993e-01  9.99999523e-01  9.91804004e-01\n",
      "   9.99798119e-01  9.99982476e-01  9.99999166e-01  9.99995470e-01\n",
      "   9.99999881e-01  9.99995351e-01  9.99999762e-01  9.99996781e-01\n",
      "   1.00000000e+00  9.99998569e-01  9.99995112e-01  3.07613701e-01\n",
      "   4.55443524e-06 -2.52607435e-07]\n",
      " [ 9.99998689e-01  3.64089727e-01  7.67333329e-01  7.44576693e-01\n",
      "   9.99737084e-01  9.95516360e-01  9.98478115e-01  9.99830365e-01\n",
      "   9.99964237e-01  9.99979496e-01  9.99881387e-01  9.99969482e-01\n",
      "   9.99614954e-01  9.99996543e-01  9.99976635e-01  9.99996901e-01\n",
      "   1.00000000e+00  9.99999642e-01  9.99776423e-01  9.99673605e-01\n",
      "   4.63519463e-08 -7.75753847e-07]\n",
      " [ 9.99278963e-01  7.58878589e-01  6.04686439e-01  8.18145335e-01\n",
      "   9.99989629e-01  9.99983668e-01  9.99847889e-01  9.99904513e-01\n",
      "   9.99729931e-01  9.99999762e-01  1.77334145e-01  9.99999166e-01\n",
      "   9.99676824e-01  9.99999762e-01  9.99926209e-01  9.99414563e-01\n",
      "   9.99998927e-01  9.99991059e-01  9.99975502e-01  9.79732037e-01\n",
      "   7.96360200e-10 -2.60720991e-08]\n",
      " [ 1.02414288e-05  3.92155198e-05  4.40225705e-08  1.12062797e-03\n",
      "   7.10790396e-01  9.85836744e-01  6.19930029e-01  9.99999762e-01\n",
      "   9.97612715e-01  6.63692713e-01  3.63748497e-03  7.78760135e-01\n",
      "   9.97967899e-01  9.99997854e-01  9.99817193e-01  9.99857664e-01\n",
      "   9.99989390e-01  9.99996066e-01  9.99984860e-01  4.32877034e-01\n",
      "   4.35954576e-07 -1.12865841e-07]\n",
      " [-3.78705609e-11 -1.96112463e-17 -1.73400747e-06 -6.98062808e-09\n",
      "   3.90324881e-03  3.70376743e-03  2.90492433e-04  5.60863256e-01\n",
      "   9.99115884e-01  1.15533755e-03  2.26953276e-07  3.50490154e-04\n",
      "   2.43574567e-02  9.63328302e-01  9.98323083e-01  9.93960679e-01\n",
      "   9.99079704e-01  9.99996781e-01  9.99999762e-01  7.58527935e-01\n",
      "   5.13812566e-08 -5.35065169e-07]\n",
      " [-9.12495161e-17 -2.62039272e-22 -8.94486314e-17 -1.83970305e-15\n",
      "  -6.11333206e-12 -6.54026380e-08 -8.14759016e-09  6.33724403e-06\n",
      "   8.52699450e-05 -5.52026904e-08 -9.93117602e-11 -3.60661667e-10\n",
      "  -1.47489201e-10  3.75698204e-04  4.44041789e-01  4.05934639e-03\n",
      "   3.89465898e-01  9.99999166e-01  9.99838352e-01  1.00000000e+00\n",
      "   1.12565471e-10 -2.19404299e-08]\n",
      " [-2.51012115e-15 -8.51721242e-18 -2.53044978e-16 -2.98547603e-19\n",
      "  -7.42747496e-19 -2.14998138e-17 -1.51316904e-12 -1.50427054e-14\n",
      "  -3.30720524e-14 -2.34471513e-13 -1.89729024e-13 -5.50767080e-16\n",
      "  -2.07387146e-11 -7.46767492e-12  8.62266404e-07 -9.14966503e-09\n",
      "   1.19854864e-11  7.38639951e-01  7.62761056e-01  1.64411291e-02\n",
      "   5.98762576e-14 -1.30502167e-06]\n",
      " [-5.53526396e-16 -3.86101569e-19 -2.58366479e-14 -1.72431175e-17\n",
      "  -5.64487117e-18 -4.04572880e-17 -1.94980039e-14 -7.98763517e-17\n",
      "  -6.19773346e-17 -1.11874163e-14 -3.28393932e-15 -2.24469314e-18\n",
      "  -1.30049691e-15 -6.13707597e-17 -7.55373056e-15 -1.50809448e-10\n",
      "  -1.68093664e-10 -2.52263314e-11  5.20678699e-01 -6.13597351e-09\n",
      "  -1.49418835e-16 -6.35057967e-03]\n",
      " [-1.46673642e-07 -1.46497977e-18 -3.25966251e-15 -1.23871874e-13\n",
      "  -8.83146875e-15 -9.38681771e-12 -6.02958217e-09 -4.38427947e-14\n",
      "  -6.03457909e-11 -2.54643702e-12 -4.14350069e-07 -3.79773817e-11\n",
      "  -2.38582274e-08 -3.32581212e-13 -4.13320392e-13 -2.32997122e-10\n",
      "  -5.46704956e-12 -9.25797741e-13 -1.74338853e-12 -1.47639842e-14\n",
      "  -3.96847421e-19 -6.31160635e-09]]\n",
      "(191, 4, 16, 22)\n",
      "(191, 1408)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read and build the neural network\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import rotate\n",
    "import imageio\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "\n",
    "model = ConvLSTM(input_dim=4, hidden_dim=[64, 64, 4], kernel_size=(3, 3), num_layers=3, batch_first=True, bias=True, return_all_layers=False)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "predicted_val = []\n",
    "\n",
    "for idx, (x, y) in enumerate(val_dataloader):\n",
    "    x = x.to(device)\n",
    "    _,out = model(x)\n",
    "    out = out[0][0].cpu()\n",
    "    predicted_val.append(out.detach().numpy()[0])\n",
    "\n",
    "# print heatmap each predicted_val\n",
    "predicted_val = np.array(predicted_val)\n",
    "print(predicted_val[0][0])\n",
    "#invert the 2d\n",
    "\n",
    "x = predicted_val[0][0]\n",
    "x = rotate(x, angle=90)\n",
    "\n",
    "plt.imshow(x, cmap='hot', interpolation='nearest')\n",
    "\n",
    "#increase the resolution of the heatmap\n",
    "new_size = (x.shape[0]*2, x.shape[1]*2)\n",
    "new_img = ndimage.zoom(x, zoom=(2, 2), order=1)\n",
    "\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(new_img, cmap='hot', interpolation='nearest')\n",
    "plt.savefig('test.png', transparent=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(predicted_val.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78620/1609649928.py:21: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread('heatmap.png'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 10\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m#add a colorbar\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m#add a progress bar to the image of the frames\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m#plt.text(0.5, 0.5, str(i), fontsize=18, ha='center')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mHeatmap of predicted values\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     plt\u001b[39m.\u001b[39;49msavefig(\u001b[39m'\u001b[39;49m\u001b[39mheatmap.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     images\u001b[39m.\u001b[39mappend(imageio\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39mheatmap.png\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m imageio\u001b[39m.\u001b[39mmimsave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./plots/heatmap_res_feature\u001b[39m\u001b[39m{\u001b[39;00mfeature_num\u001b[39m}\u001b[39;00m\u001b[39m.gif\u001b[39m\u001b[39m'\u001b[39m, images)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/pyplot.py:954\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[39m.\u001b[39msavefig)\n\u001b[1;32m    952\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msavefig\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    953\u001b[0m     fig \u001b[39m=\u001b[39m gcf()\n\u001b[0;32m--> 954\u001b[0m     res \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39;49msavefig(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    955\u001b[0m     fig\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mdraw_idle()  \u001b[39m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/figure.py:3274\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3270\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes:\n\u001b[1;32m   3271\u001b[0m         stack\u001b[39m.\u001b[39menter_context(\n\u001b[1;32m   3272\u001b[0m             ax\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39m_cm_set(facecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m, edgecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m-> 3274\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/backend_bases.py:2338\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2335\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2336\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2337\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m-> 2338\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2339\u001b[0m             filename,\n\u001b[1;32m   2340\u001b[0m             facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2341\u001b[0m             edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2342\u001b[0m             orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2343\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2344\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2345\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2346\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2201\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   2203\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[0;32m-> 2204\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[1;32m   2205\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m skip}))\n\u001b[1;32m   2206\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/_api/deprecation.py:410\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     deprecation_addendum \u001b[39m=\u001b[39m (\n\u001b[1;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIf any parameter follows \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m, they should be passed as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mkeyword, not positionally.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    403\u001b[0m     warn_deprecated(\n\u001b[1;32m    404\u001b[0m         since,\n\u001b[1;32m    405\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39mrepr\u001b[39m(name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m                  \u001b[39melse\u001b[39;00m deprecation_addendum,\n\u001b[1;32m    409\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 410\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49minner_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minner_kwargs)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:517\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m@_api\u001b[39m\u001b[39m.\u001b[39mdelete_parameter(\u001b[39m\"\u001b[39m\u001b[39m3.5\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39margs,\n\u001b[1;32m    470\u001b[0m               metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    471\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:463\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_print_pil\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    459\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[39m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[39m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     FigureCanvasAgg\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    464\u001b[0m     mpl\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimsave(\n\u001b[1;32m    465\u001b[0m         filename_or_obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_rgba(), \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mfmt, origin\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupper\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    466\u001b[0m         dpi\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mdpi, metadata\u001b[39m=\u001b[39mmetadata, pil_kwargs\u001b[39m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:405\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[1;32m    403\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[1;32m    404\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 405\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[1;32m    406\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 74\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     76\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/figure.py:3071\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3068\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3070\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3071\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3072\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3074\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   3075\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/axes/_base.py:3107\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3104\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   3105\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 3107\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3108\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3110\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3111\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/image.py:641\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[1;32m    640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 641\u001b[0m     im, l, b, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_image(\n\u001b[1;32m    642\u001b[0m         renderer, renderer\u001b[39m.\u001b[39;49mget_image_magnification())\n\u001b[1;32m    643\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/image.py:949\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    946\u001b[0m transformed_bbox \u001b[39m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[1;32m    947\u001b[0m clip \u001b[39m=\u001b[39m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mbbox) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on()\n\u001b[1;32m    948\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mbbox)\n\u001b[0;32m--> 949\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_A, bbox, transformed_bbox, clip,\n\u001b[1;32m    950\u001b[0m                         magnification, unsampled\u001b[39m=\u001b[39;49munsampled)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/image.py:366\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mif\u001b[39;00m A\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    362\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m_make_image must get a non-empty image. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mYour Artist\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms draw method must filter before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mthis method is called.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 366\u001b[0m clipped_bbox \u001b[39m=\u001b[39m Bbox\u001b[39m.\u001b[39;49mintersection(out_bbox, clip_bbox)\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m clipped_bbox \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/transforms.py:666\u001b[0m, in \u001b[0;36mBboxBase.intersection\u001b[0;34m(bbox1, bbox2)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mintersection\u001b[39m(bbox1, bbox2):\n\u001b[1;32m    662\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39m    Return the intersection of *bbox1* and *bbox2* if they intersect, or\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[39m    None if they don't.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     x0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(bbox1\u001b[39m.\u001b[39;49mxmin, bbox2\u001b[39m.\u001b[39mxmin)\n\u001b[1;32m    667\u001b[0m     x1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mminimum(bbox1\u001b[39m.\u001b[39mxmax, bbox2\u001b[39m.\u001b[39mxmax)\n\u001b[1;32m    668\u001b[0m     y0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(bbox1\u001b[39m.\u001b[39mymin, bbox2\u001b[39m.\u001b[39mymin)\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/transforms.py:316\u001b[0m, in \u001b[0;36mBboxBase.xmin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mxmin\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    315\u001b[0m     \u001b[39m\"\"\"The left edge of the bounding box.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_points()[:, \u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/matplotlib/transforms.py:1116\u001b[0m, in \u001b[0;36mTransformedBbox.get_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1112\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bbox\u001b[39m.\u001b[39mget_points()\n\u001b[1;32m   1113\u001b[0m \u001b[39m# Transform all four points, then make a new bounding box\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# from the result, taking care to make the orientation the\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39m# same.\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform\u001b[39m.\u001b[39;49mtransform(\n\u001b[1;32m   1117\u001b[0m     [[p[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m], p[\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m]],\n\u001b[1;32m   1118\u001b[0m      [p[\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m], p[\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m]],\n\u001b[1;32m   1119\u001b[0m      [p[\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m], p[\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m]],\n\u001b[1;32m   1120\u001b[0m      [p[\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m], p[\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m]]])\n\u001b[1;32m   1121\u001b[0m points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mma\u001b[39m.\u001b[39mfilled(points, \u001b[39m0.0\u001b[39m)\n\u001b[1;32m   1123\u001b[0m xs \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(points[:, \u001b[39m0\u001b[39m]), \u001b[39mmax\u001b[39m(points[:, \u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = []\n",
    "\n",
    "feature_num = 2\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 191):\n",
    "    x = predicted_val[i][feature_num]\n",
    "    x = rotate(x, angle=90)\n",
    "    new_size = (x.shape[0]*2, x.shape[1]*2)\n",
    "    new_img = ndimage.zoom(x, zoom=(2, 2), order=1) \n",
    "    plt.imshow(new_img, cmap='hot_r', interpolation='nearest')\n",
    "    #add a colorbar\n",
    "    #add a progress bar to the image of the frames\n",
    "    #plt.text(0.5, 0.5, str(i), fontsize=18, ha='center')\n",
    "    plt.title('Heatmap of predicted values')\n",
    "    \n",
    "\n",
    "\n",
    "    plt.savefig('heatmap.png')\n",
    "    images.append(imageio.imread('heatmap.png'))\n",
    "\n",
    "imageio.mimsave(f'./plots/heatmap_feature{feature_num}.gif', images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
