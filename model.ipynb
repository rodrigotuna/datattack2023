{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_scaled_error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvLSTMCell** is the basic building block of the ConvLSTM model. It defines a single ConvLSTM cell that takes as input a tensor and a tuple representing the current state of the cell, and outputs a tensor and a tuple representing the next state of the cell. The cell includes a convolutional layer and four gates (input, forget, output, and cell gates) that are used to update the cell state and hidden state. The output tensor is the hidden state of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvLSTM** is the main class that creates a ConvLSTM model with multiple layers. It takes as input a tensor and an optional tuple representing the initial hidden state of the model, and returns a tuple of two lists. The first list contains the hidden states of each layer at each timestep, and the second list contains the final hidden states and cell states of each layer. The model includes multiple ConvLSTMCell cells that are stacked on top of each other to create a deeper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    channels = x.shape[1]\n",
    "    width = x.shape[2]\n",
    "    height = x.shape[3]\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row,window_size, channels, width, height), strides=(x.strides[0],x.strides[0],x.strides[1],x.strides[2],x.strides[3]))\n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "def prepare_data(normalized_data_close_price):\n",
    "    data_x, _ = prepare_data_x(normalized_data_close_price, window_size=12)\n",
    "    data_y = prepare_data_y(normalized_data_close_price, window_size=12)\n",
    "\n",
    "    # split dataset\n",
    "\n",
    "    split_index = int(data_y.shape[0]*0.8)\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "\n",
    "    return split_index, data_x_train, data_y_train, data_x_val, data_y_val\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m train_model(model, dataloader, criterion, optimizer, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m predicted_val \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/home/sergio/Documents/feup/eren-yeager/model.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m _, output \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/Documents/feup/eren-yeager/model.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/env/feup/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        # extract date string from row and parse it as a datetime objsaDect\n",
    "        date_str = row[1]\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "        features = row[3:].values.astype(np.float32)\n",
    "        label = row[0].astype(np.float32)\n",
    "        sample = {'date_begin': date, 'features': self.transform(features), 'label': label}\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Define the training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for idx, (x,y) in enumerate(dataloader,0):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, output = model(x)\n",
    "            loss = criterion(output[0][0], y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch %d, loss: %.3f' % (epoch + 1, running_loss / len(dataloader)))\n",
    "\n",
    "# Load the data\n",
    "dataset = MyDataset('./data/ocorrencias_final.csv')\n",
    "data_grid = np.load('data_grid.npy') \n",
    "data_grid = data_grid.reshape(965, 22, 16, 4)\n",
    "data_grid = np.swapaxes(data_grid, 3, 1)\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "split_index, data_x_train, data_y_train, data_x_val, data_y_val = prepare_data(data_grid)\n",
    "\n",
    "\n",
    "dataset_train = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "dataset_val = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "\n",
    "data_x_train = data_x_train.astype(np.float32)\n",
    "data_y_train = data_y_train.astype(np.float32)\n",
    "\n",
    "dataloader = DataLoader(dataset_train, batch_size=1, shuffle=False, num_workers=12)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=12)\n",
    "\n",
    "# Create the model, optimizer and loss function\n",
    "model = ConvLSTM(input_dim=4, hidden_dim=[64, 64, 4], kernel_size=(3, 3), num_layers=3, batch_first=True, bias=True, return_all_layers=False)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.004)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predicted_val = []\n",
    "\n",
    "for idx, (x, y) in enumerate(val_dataloader):\n",
    "    x = x.to(device)\n",
    "    _,out = model(x)\n",
    "    out = out[0][0].cpu()\n",
    "    predicted_val.append(out.detach().numpy()[0])\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "predicted_val = np.array(predicted_val)\n",
    "print(predicted_val.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "data_y_val = data_y_val.reshape(191, 1408)\n",
    "predicted_val = predicted_val.reshape(191, 1408)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(data_y_val, predicted_val))\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 1408)\n",
      "(191, 1408)\n",
      "13.398050417386921\n",
      "1.4089250395538317\n"
     ]
    }
   ],
   "source": [
    "predicted_val = np.array(predicted_val)\n",
    "print(predicted_val.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "data_y_val = data_y_val.reshape(191, 1408)\n",
    "predicted_val = predicted_val.reshape(191, 1408)\n",
    "data_y_train = data_y_train.reshape(762, 1408)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(data_y_val, predicted_val))\n",
    "mase = mean_absolute_scaled_error(y_true=data_y_val, y_pred=predicted_val, y_train=data_y_train)\n",
    "print(rmse)\n",
    "print(mase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and build the neural network\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import rotate\n",
    "import imageio\n",
    "\n",
    "\n",
    "model = ConvLSTM(input_dim=4, hidden_dim=[64, 64, 4], kernel_size=(3, 3), num_layers=3, batch_first=True, bias=True, return_all_layers=False)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "predicted_val = []\n",
    "\n",
    "for idx, (x, y) in enumerate(val_dataloader):\n",
    "    x = x.to(device)\n",
    "    _,out = model(x)\n",
    "    out = out[0][0].cpu()\n",
    "    predicted_val.append(out.detach().numpy()[0])\n",
    "\n",
    "# print heatmap each predicted_val\n",
    "predicted_val = np.array(predicted_val)\n",
    "print(predicted_val[0][0])\n",
    "#invert the 2d\n",
    "\n",
    "x = predicted_val[0][0]\n",
    "x = rotate(x, angle=90)\n",
    "\n",
    "plt.imshow(x, cmap='hot', interpolation='nearest')\n",
    "#rotate the axis\n",
    "\n",
    "#create a gif with the heatmap using imageio and matplotlib\n",
    "\n",
    "images = []\n",
    "for i in range(0, 191):\n",
    "    x = predicted_val[i][0]\n",
    "    x = rotate(x, angle=90)\n",
    "    plt.imshow(x, cmap='hot', interpolation='nearest')\n",
    "    #add a colorbar\n",
    "    plt.colorbar()\n",
    "    #add a progress bar to the image of the frames\n",
    "    plt.text(0.5, 0.5, str(i), fontsize=18, ha='center')\n",
    "    plt.title('Heatmap of predicted values')\n",
    "    \n",
    "\n",
    "\n",
    "    plt.savefig('heatmap.png')\n",
    "    images.append(imageio.imread('heatmap.png'))\n",
    "\n",
    "imageio.mimsave('heatmap.gif', images)\n",
    "\n",
    "\n",
    "\n",
    "print(predicted_val.shape)\n",
    "print(data_y_val.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8799/2145139779.py:17: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread('heatmap.png'))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = []\n",
    "\n",
    "feature_num = 3\n",
    "\n",
    "for i in range(0, 191):\n",
    "    x = predicted_val[i][feature_num]\n",
    "    x = rotate(x, angle=90)\n",
    "    plt.imshow(x, cmap='hot', interpolation='nearest')\n",
    "    #add a colorbar\n",
    "    #add a progress bar to the image of the frames\n",
    "    #plt.text(0.5, 0.5, str(i), fontsize=18, ha='center')\n",
    "    plt.title('Heatmap of predicted values')\n",
    "    \n",
    "\n",
    "\n",
    "    plt.savefig('heatmap.png')\n",
    "    images.append(imageio.imread('heatmap.png'))\n",
    "\n",
    "imageio.mimsave(f'./plots/heatmap_feature{feature_num}.gif', images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
